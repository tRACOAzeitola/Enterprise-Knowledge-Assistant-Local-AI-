{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Enterprise Knowledge Assistant (Local LLaMA + RAG)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install langchain chromadb pypdf pymupdf pillow gradio numpy langchain-ollama langchain-community"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import fitz  # PyMuPDF\n",
                "from PIL import Image as PILImage\n",
                "\n",
                "import gradio as gr\n",
                "\n",
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_community.vectorstores import Chroma\n",
                "\n",
                "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
                "\n",
                "from langchain_core.prompts import PromptTemplate\n",
                "try:\n",
                "    from langchain.chains import RetrievalQA\n",
                "except ImportError:\n",
                "    from langchain_classic.chains import RetrievalQA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Modelo LLaMA local via Ollama\n",
                "LLM_MODEL_NAME = \"llama3:8b\"  # ajusta se usares outro nome/modelo\n",
                "\n",
                "llm = ChatOllama(\n",
                "    model=LLM_MODEL_NAME,\n",
                "    temperature=0.2\n",
                ")\n",
                "\n",
                "# Embeddings locais via Ollama\n",
                "embeddings = OllamaEmbeddings(\n",
                "    model=LLM_MODEL_NAME\n",
                ")\n",
                "\n",
                "print(\"‚úÖ LLM e embeddings locais configurados.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BASE_DIR = \"data\"\n",
                "DB_DIR = \"db\"\n",
                "\n",
                "CATEGORIES = {\n",
                "    \"Informa√ß√µes Internas\": \"internas\",\n",
                "    \"Pe√ßas / Material\": \"pecas\",\n",
                "    \"Manuais\": \"manuais\",\n",
                "    \"Outros\": \"outros\"\n",
                "}\n",
                "\n",
                "os.makedirs(BASE_DIR, exist_ok=True)\n",
                "os.makedirs(DB_DIR, exist_ok=True)\n",
                "\n",
                "print(\"Categorias dispon√≠veis:\", list(CATEGORIES.keys()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_vector_db_for_category(category_name):\n",
                "    folder_key = CATEGORIES[category_name]\n",
                "    pdf_folder = os.path.join(BASE_DIR, folder_key)\n",
                "    db_folder = os.path.join(DB_DIR, folder_key)\n",
                "\n",
                "    os.makedirs(db_folder, exist_ok=True)\n",
                "\n",
                "    if not os.path.isdir(pdf_folder):\n",
                "        print(f\"‚ö†Ô∏è Pasta n√£o encontrada para categoria '{category_name}': {pdf_folder}\")\n",
                "        return\n",
                "\n",
                "    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
                "\n",
                "    if not pdf_files:\n",
                "        print(f\"‚ö†Ô∏è Nenhum PDF encontrado em {pdf_folder}\")\n",
                "        return\n",
                "\n",
                "    all_docs = []\n",
                "\n",
                "    for pdf in pdf_files:\n",
                "        path = os.path.join(pdf_folder, pdf)\n",
                "        loader = PyPDFLoader(path)\n",
                "        docs = loader.load()\n",
                "\n",
                "        for d in docs:\n",
                "            d.metadata[\"source\"] = pdf\n",
                "            d.metadata[\"category\"] = category_name\n",
                "\n",
                "        all_docs.extend(docs)\n",
                "\n",
                "    splitter = RecursiveCharacterTextSplitter(\n",
                "        chunk_size=1000,\n",
                "        chunk_overlap=200,\n",
                "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\"]\n",
                "    )\n",
                "\n",
                "    chunks = splitter.split_documents(all_docs)\n",
                "\n",
                "    vector_store = Chroma.from_documents(\n",
                "        documents=chunks,\n",
                "        embedding=embeddings,\n",
                "        persist_directory=db_folder\n",
                "    )\n",
                "\n",
                "    # vector_store.persist() # In newer versions of Chroma, persist is automatic or handled differently, but keeping for compatibility if old version used\n",
                "    print(f\"‚úÖ Base vetorial criada para categoria: {category_name} ({len(chunks)} chunks)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for cat in CATEGORIES.keys():\n",
                "    create_vector_db_for_category(cat)\n",
                "\n",
                "print(\"‚úÖ Todas as bases vetoriais foram processadas (se havia PDFs).\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_vector_store(category_name):\n",
                "    folder_key = CATEGORIES[category_name]\n",
                "    db_folder = os.path.join(DB_DIR, folder_key)\n",
                "\n",
                "    if not os.path.isdir(db_folder):\n",
                "        raise ValueError(f\"Base vetorial n√£o encontrada para categoria '{category_name}' em {db_folder}\")\n",
                "\n",
                "    vs = Chroma(\n",
                "        embedding_function=embeddings,\n",
                "        persist_directory=db_folder\n",
                "    )\n",
                "    return vs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "REJECTION_PHRASE = \"N√£o encontro essa informa√ß√£o nos documentos dispon√≠veis\"\n",
                "\n",
                "prompt_template = \"\"\"\n",
                "√âs um assistente inteligente especializado em gest√£o de conhecimento empresarial.\n",
                "\n",
                "Regras:\n",
                "- Responde APENAS com base na informa√ß√£o dos documentos fornecidos.\n",
                "- Se a resposta n√£o estiver clara nos documentos, responde exatamente: \"{rejection_phrase}\".\n",
                "- Usa linguagem profissional, corporativa e direta.\n",
                "- Sempre que fizer sentido, organiza a resposta em pontos ou par√°grafos curtos.\n",
                "\n",
                "Categoria selecionada: {category}\n",
                "\n",
                "Contexto (excerto dos documentos internos):\n",
                "{context}\n",
                "\n",
                "Pergunta do utilizador:\n",
                "{question}\n",
                "\n",
                "Resposta:\n",
                "\"\"\"\n",
                "\n",
                "qa_prompt = PromptTemplate(\n",
                "    input_variables=[\"context\", \"question\", \"category\", \"rejection_phrase\"],\n",
                "    template=prompt_template\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_rag_chain_for_category(category_name):\n",
                "    vector_store = load_vector_store(category_name)\n",
                "\n",
                "    retriever = vector_store.as_retriever(\n",
                "        search_kwargs={\"k\": 5}\n",
                "    )\n",
                "\n",
                "    chain = RetrievalQA.from_chain_type(\n",
                "        llm=llm,\n",
                "        chain_type=\"stuff\",\n",
                "        retriever=retriever,\n",
                "        return_source_documents=True,\n",
                "        chain_type_kwargs={\n",
                "            \"prompt\": qa_prompt.partial(\n",
                "                category=category_name,\n",
                "                rejection_phrase=REJECTION_PHRASE\n",
                "            )\n",
                "        }\n",
                "    )\n",
                "    return chain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def answer_question(category_name, user_question):\n",
                "    if not user_question:\n",
                "        return \"Por favor, insira uma pergunta.\"\n",
                "\n",
                "    try:\n",
                "        qa_chain = create_rag_chain_for_category(category_name)\n",
                "    except Exception as e:\n",
                "        return f\"Erro ao preparar o motor de pesquisa para a categoria '{category_name}': {e}\"\n",
                "\n",
                "    try:\n",
                "        result = qa_chain.invoke({\"query\": user_question})\n",
                "        answer_text = result[\"result\"]\n",
                "        source_docs = result.get(\"source_documents\", [])\n",
                "\n",
                "        # Acrescentar fontes no fim da resposta\n",
                "        if source_docs:\n",
                "            answer_text += \"\\n\\nFontes consultadas:\"\n",
                "            unique_sources = set()\n",
                "            for doc in source_docs:\n",
                "                src = doc.metadata.get(\"source\", \"desconhecido\")\n",
                "                unique_sources.add(src)\n",
                "            for src in unique_sources:\n",
                "                answer_text += f\"\\n- {src}\"\n",
                "\n",
                "        return answer_text\n",
                "\n",
                "    except Exception as e:\n",
                "        return f\"Ocorreu um erro ao consultar o modelo local: {e}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gradio_answer(category, question):\n",
                "    return answer_question(category, question)\n",
                "\n",
                "with gr.Blocks(title=\"ü§ñ Enterprise Knowledge Assistant\") as demo:\n",
                "    gr.Markdown(\"\"\"\n",
                "    # ü§ñ Enterprise Knowledge Assistant (Local Secure AI)\n",
                "    Assistente inteligente para consulta de documenta√ß√£o interna.\n",
                "    Seguran√ßa total: todas as respostas s√£o geradas localmente (LLaMA 3) sem envio de dados para a cloud.\n",
                "    \"\"\")\n",
                "\n",
                "    with gr.Row():\n",
                "        category_input = gr.Dropdown(\n",
                "            choices=list(CATEGORIES.keys()),\n",
                "            value=\"Informa√ß√µes Internas\",\n",
                "            label=\"Categoria de documentos\"\n",
                "        )\n",
                "        question_input = gr.Textbox(\n",
                "            lines=4,\n",
                "            label=\"Pergunta\",\n",
                "            placeholder=\"Ex.: Qual √© a pol√≠tica de f√©rias? / Como √© o procedimento de seguran√ßa X?\"\n",
                "        )\n",
                "\n",
                "    submit_btn = gr.Button(\"Obter resposta\", variant=\"primary\")\n",
                "\n",
                "    answer_output = gr.Textbox(\n",
                "        lines=12,\n",
                "        label=\"Resposta do assistente\"\n",
                "    )\n",
                "\n",
                "    submit_btn.click(\n",
                "        fn=gradio_answer,\n",
                "        inputs=[category_input, question_input],\n",
                "        outputs=[answer_output]\n",
                "    )\n",
                "\n",
                "demo.launch()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}